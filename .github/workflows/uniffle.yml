#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

name: Uniffle

on:
  workflow_dispatch:
  pull_request:
    branches:
      - master
      - branch-*

concurrency:
  group: uniffle-${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  test-uniffle:
    strategy:
      fail-fast: false
      matrix:
        include:
          - unifflever: "0.9.2"
            hadoopver: "2.8.5"
            uniffleprofile: "uniffle,uniffle-0.9"
    uses: ./.github/workflows/tpcds-reusable.yml
    name: Test Uniffle ${{ matrix.unifflever }}
    with:
      unifflever: ${{ matrix.unifflever }}
      uniffleurl: https://archive.apache.org/dist/uniffle/${{ matrix.unifflever }}/apache-uniffle-${{ matrix.unifflever }}-incubating-bin.tar.gz
      hadoopver: ${{ matrix.hadoopver }}
      hadoopurl: https://archive.apache.org/dist/hadoop/common/hadoop-${{ matrix.hadoopver }}/hadoop-${{ matrix.hadoopver }}.tar.gz
      extrabuildopt: -P${{ matrix.uniffleprofile }} -DuniffleVersion=${{ matrix.unifflever }}
      extraidentifier: uniffle-${{ matrix.unifflever }}
      sparkver: "spark-3.5"
      sparkurl: "https://archive.apache.org/dist/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz"
      scalaver: "2.12"
      extrasparkconf: >-
        --conf spark.shuffle.manager=org.apache.spark.sql.execution.auron.shuffle.uniffle.AuronUniffleShuffleManager
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
        --conf spark.rss.coordinator.quorum=localhost:19999
        --conf spark.rss.enabled=true
        --conf spark.rss.storage.type=MEMORY_LOCALFILE
        --conf spark.rss.client.type=GRPC_NETTY
      queries: '["q1,q2,q3,q4,q5,q6,q7,q8,q9"]'
